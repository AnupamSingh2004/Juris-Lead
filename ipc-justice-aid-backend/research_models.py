#!/usr/bin/env python3
"""
Research script to find the best models for legal analysis
"""
import requests
import json

def search_huggingface_models():
    """Search for legal and instruction-following models on Hugging Face"""
    
    print("üîç Researching Best Models for Legal Analysis")
    print("=" * 60)
    
    # Categories of models that would work well for legal analysis
    model_categories = {
        "Legal-Specific Models": [
            "nlpaueb/legal-bert-base-uncased",
            "zlucia/custom-legalbert", 
            "law-ai/InLegalBERT",
            "lexlms/legal-bert-base",
            "saradhix/legal_bert_uncased",
            "pile-of-law/legalbert-large-1.7M-2",
        ],
        
        "Large Instruction Models (Good for Legal Reasoning)": [
            # Llama 2 models
            "meta-llama/Llama-2-7b-chat-hf",
            "meta-llama/Llama-2-13b-chat-hf", 
            "meta-llama/Llama-2-70b-chat-hf",
            
            # Mistral models  
            "mistralai/Mistral-7B-Instruct-v0.1",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "mistralai/Mixtral-8x7B-Instruct-v0.1",
            
            # Code Llama (good for structured output)
            "codellama/CodeLlama-7b-Instruct-hf",
            "codellama/CodeLlama-13b-Instruct-hf",
            
            # Vicuna models
            "lmsys/vicuna-7b-v1.5",
            "lmsys/vicuna-13b-v1.5",
            
            # Zephyr models
            "HuggingFaceH4/zephyr-7b-beta",
            "HuggingFaceH4/zephyr-7b-alpha",
        ],
        
        "Large General Models": [
            # Google models
            "google/flan-t5-large",
            "google/flan-t5-xl", 
            "google/flan-t5-xxl",
            "google/flan-ul2",
            
            # Falcon models
            "tiiuae/falcon-7b-instruct",
            "tiiuae/falcon-40b-instruct",
            
            # MPT models
            "mosaicml/mpt-7b-instruct",
            "mosaicml/mpt-30b-instruct",
        ],
        
        "Qwen Models (Latest)": [
            "Qwen/Qwen-7B-Chat",
            "Qwen/Qwen-14B-Chat", 
            "Qwen/Qwen-72B-Chat",
            "Qwen/Qwen1.5-7B-Chat",
            "Qwen/Qwen1.5-14B-Chat",
            "Qwen/Qwen1.5-72B-Chat",
            "Qwen/Qwen2-7B-Instruct",
            "Qwen/Qwen2-72B-Instruct",
            "Qwen/Qwen3-30B-A3B-Instruct-2507",  # The one you wanted to try
        ]
    }
    
    print("üìä RECOMMENDED MODELS FOR LEGAL ANALYSIS")
    print("=" * 60)
    
    for category, models in model_categories.items():
        print(f"\nüè∑Ô∏è  {category}")
        print("-" * 40)
        for model in models:
            print(f"   ‚Ä¢ {model}")
    
    print("\n" + "=" * 60)
    print("üéØ TOP RECOMMENDATIONS FOR YOUR USE CASE")
    print("=" * 60)
    
    recommendations = [
        {
            "model": "mistralai/Mistral-7B-Instruct-v0.2",
            "pros": "Best balance of performance and size, excellent instruction following, good for legal reasoning",
            "cons": "Requires Pro access on HF",
            "size": "7B parameters"
        },
        {
            "model": "meta-llama/Llama-2-7b-chat-hf", 
            "pros": "Excellent for structured output, trained on diverse data including legal texts",
            "cons": "Requires Pro access, may need approval",
            "size": "7B parameters"
        },
        {
            "model": "google/flan-t5-xl",
            "pros": "Good instruction following, might be available on free tier",
            "cons": "Smaller than modern models",
            "size": "3B parameters"
        },
        {
            "model": "Qwen/Qwen2-7B-Instruct",
            "pros": "Latest generation, excellent performance, good multilingual support",
            "cons": "Requires Pro access",
            "size": "7B parameters"
        },
        {
            "model": "HuggingFaceH4/zephyr-7b-beta",
            "pros": "Open source, good instruction following, optimized for helpfulness",
            "cons": "May require Pro access",
            "size": "7B parameters"
        }
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. {rec['model']}")
        print(f"   üìè Size: {rec['size']}")
        print(f"   ‚úÖ Pros: {rec['pros']}")
        print(f"   ‚ö†Ô∏è  Cons: {rec['cons']}")
    
    print("\n" + "=" * 60)
    print("üí° SOLUTIONS FOR MODEL ACCESS")
    print("=" * 60)
    
    solutions = [
        "1. üîì Upgrade to Hugging Face Pro ($20/month)",
        "   ‚Ä¢ Access to all Inference API models",
        "   ‚Ä¢ Higher rate limits", 
        "   ‚Ä¢ Priority support",
        "",
        "2. üè† Self-hosted deployment:",
        "   ‚Ä¢ Use Ollama locally for development",
        "   ‚Ä¢ Deploy your own model on cloud (AWS, GCP, Azure)",
        "   ‚Ä¢ Use vLLM or TGI for serving",
        "",
        "3. üîÑ Alternative APIs:",
        "   ‚Ä¢ OpenAI GPT-4 (excellent for legal analysis)",
        "   ‚Ä¢ Anthropic Claude (very good at following instructions)",
        "   ‚Ä¢ Google Gemini Pro",
        "   ‚Ä¢ Groq (fast inference with Llama/Mistral)",
        "",
        "4. üìù Use available model creatively:",
        "   ‚Ä¢ Adapt facebook/bart-large-cnn for legal summarization",
        "   ‚Ä¢ Chain multiple smaller models",
        "   ‚Ä¢ Use it for document summarization + rule-based classification"
    ]
    
    for solution in solutions:
        print(solution)
    
    print("\n" + "=" * 60)
    print("üöÄ IMMEDIATE ACTION PLAN")
    print("=" * 60)
    
    action_plan = [
        "1. üí≥ Upgrade to HuggingFace Pro for best experience",
        "2. üéØ Use Mistral-7B-Instruct-v0.2 as primary model", 
        "3. üîÑ Keep Ollama as local development fallback",
        "4. üìä Test with smaller models if budget is constrained",
        "5. üèóÔ∏è  Consider hybrid approach: HF for production, Ollama for dev"
    ]
    
    for action in action_plan:
        print(action)

def check_huggingface_pricing():
    """Display current Hugging Face pricing information"""
    print("\n" + "=" * 60)
    print("üí∞ HUGGING FACE PRICING (as of 2024)")
    print("=" * 60)
    
    pricing = {
        "Free Tier": {
            "price": "$0/month",
            "features": [
                "Limited model access",
                "Rate limited API calls", 
                "Basic support",
                "Some models unavailable"
            ]
        },
        "Pro Tier": {
            "price": "$20/month", 
            "features": [
                "Access to all Inference API models",
                "Higher rate limits",
                "Priority support",
                "Early access to new models",
                "Spaces Pro features"
            ]
        },
        "Enterprise": {
            "price": "Custom pricing",
            "features": [
                "Dedicated infrastructure",
                "SLA guarantees", 
                "Custom model hosting",
                "Advanced security"
            ]
        }
    }
    
    for tier, info in pricing.items():
        print(f"\nüè∑Ô∏è  {tier}: {info['price']}")
        for feature in info['features']:
            print(f"   ‚Ä¢ {feature}")

if __name__ == "__main__":
    search_huggingface_models()
    check_huggingface_pricing()
    
    print("\n" + "=" * 60)
    print("üîó USEFUL LINKS")
    print("=" * 60)
    print("‚Ä¢ HuggingFace Pro: https://huggingface.co/pricing")
    print("‚Ä¢ Model Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard")
    print("‚Ä¢ Legal AI Models: https://huggingface.co/models?search=legal")
    print("‚Ä¢ Inference API Docs: https://huggingface.co/docs/api-inference/index")
